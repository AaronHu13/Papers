**YOLO**
===
*概述*
    
    提出一种全新的网络架构，通过end-end的物体检测方式（真正的将bounding box的生
    产与最终预测结果的生成整合到一起，不需要分别训练），取得更快的速度，且简化
    了网络结构。

*具体实现*

    1.将原图分割为S*S的网格。如果某目标的中心落入某个格子，该格子负责检测该目标

    2.每个网格需要预测B个Bounding Boxes以及上限为C个物体类别的概率（但在后续的
    操作中，会选取概率最大的物体作为该格子预测的物体概率，即一个格子只负责一种
    物体）

    3.每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。
    该置信度一方面表明该格子中是否有物体，另一方面表明对该物体预测的可信度（这个好理解）


*优势与创新点*

    1. 构建了一个完整的end-end的CNN，完成从原始输入到物体位置与类别的输出。与
    Faster-RCNN的轮流训练方法不同（Faster-RCNN需要分别训练region proposal
    与RoI层，进行两个轮回），其只需要作为一个整体作为训练即可
    
    2. 速度提升明显，相较于Faster-RCNN的两步走战略，其只需要一次处理就能得到所需要的数据

*缺点*
    
    以下摘自论文：
    1.一个格子只会生成2个bounding box，且只会对应一个class，所以对于聚
    集在一起的物体（如一大群鸟），就不能做很好的识别。

    2.bounding box的生成从数据中学来，所以会不可避免的对新的或者不寻常的物体
    做很好的预测，此外由于引进了大量的下采样层，所以其相对来说只能学到一些比较
    基础的特征

    3.系统对不同大小的bounding box中所产生的误差采取相同的处理，所以不可避免
    的会提高小bounding box的误差率

    致命缺点：准确率不高

*运行速度*

    速度是真的快。在realtime的领域，准确率吊打对手
![YOLO速度][01]
[01]: https://pic3.zhimg.com/v2-1f4454780e8075d98493222548b5e4ca_b.png "运行速度"  


*想法与疑惑*

    1.一开始的时候错以为该网格预测的box的大小仅局限在这个格子里，所以一直搞不明
    白怎么能够识别一个完整的物体，后来才明白应该是以该格子为中心，类似于Anchor
    那样，然后开始检测。这样的话，那就算是我在读完Faster-RCNN后提出设想的一种实
    现:以一定间隔的像素点为基础生成bounding box，从而实现加速

    2.原文中没有说清楚这个“如果某目标的中心落入某个格子，该格子负责检测该目标”
    是属于哪一个阶段的操作。查了各种资料也没有看到相关的解释。斗胆推测，这个应
    该是test阶段的操作吧。因为train阶段所有的格子都会开始生成bounding box，单独
    挑出来一个好像也没什么用

    3. 文中说到这是首次将object detection作为一种回归问题进行求解。但表示不是很
    明白回归问题与分类+回归的处理方式（RCNN系列）有什么本质的区别吗？RCNN不也就
    多了一个“这是不是一个物体”的判断嘛。后续的操作感觉一样啊（RoI层根据feature 
    map与proposal region 生成bounding box，训练过程中需要学习怎么进
    行平移与缩放）。 而YOLO则是每个格子先生成一个候选框，然后结合GT进行学习拟合
    操作。表示看不出两者有什么非常大的区别。



**SSD**
===

*概述*

    提出使用单个CNN进行object detection的方法，消除了region proposal的过程，其
    处理方式类似YOLO，但相较前者，其准确率得到明显提升，且可以适应各种尺寸的对
    象。对于VOC2007，在500*500的输入下达到75.1%的mAP，比肩Faster-CNN。


*具体实现*


    将卷积特征层添加到截断的基础网络的末尾（如VGG-16），通过尺寸逐渐减小的卷
    积特征层，得到多个尺度检测的预测值，综合这些预测结果进行object detection。

    训练：
        匹配策略：
        寻找与ground truth box有最大重叠度的default box，从而保证每
        个GT都有一个唯一的box与之匹配，

        其余的只要重叠度大于50%的，也都会标注为match；小于50%的，标
        注为负样本

        Hard negative mining：
        大多数的默认框都是负样本，当可能的默认框很多时，这会导致训练时正负样本
        之间的不平衡。故而根据最高置信度进行排序并选取较高的数据，使正负样本之
        间的比例为3:1，这将有助于得到更快、更稳定的训练

        data augment：
        采集原始图像的一个片段，翻转并调整为固定大小


*创新点与优势*

    优势：运行速度媲美YOLO，准确率媲美Faster-RCNN

    创新点：
    1.Faster-RCNN的Anchor与YOLO的基本结构相结合
    2.用不断减小的卷积层对多个feature map进行bounding box的提取（觉得这个真的挺重要。尺度不断减小是因为越到后面卷积图片大小越小，用大的尺度得不到比较好的结果; 而不同阶段的卷积结果，都会带有一些不同的特征。相比于Faster-RCNN只使用在进行十几次卷积后的feature map，SSD综合了各个阶段的特征图，故而能够得到更好的训练结果也是完全可以理解的）

*缺点*

    需要手动设置prior box的min_size, max_size与aspect_ratio,从而使得调试非常依赖经验

    对于较小的图片，还是没办法生成较好的bounding box（较小的图片在最
    后几层的卷积过程中，生成不了多少非常有价值的feature map了）

*一点想法*

    1. 感觉Faster-RCNN的准确率基本是目前的最高水平了，想要取得又快又好的模型，
    应该是以Faster-RCNN的核心思想为基础（如SSD使用了Anchor），进行加速。如果能
    把分类重新引入到SSD或者YOLO中，不知道准确率能否有再进一步的提升。

    2. 在阅读完这两篇论文之后，确实感受到了这种“用单一的，整体的网络来解决问题”
    的趋势。Faster-RCNN等于是两个CNN拼接起来的，而YOLO与SSD则完全是一个完整的
    CNN执行任务，其训练都是完全统一的，不需要像Faster-RCNN那样轮流训练


**ResNet**
===

*概述*

    普遍认为更深层次的网络可以取得更好的性能，但事实上过深的网络会引入超多的参
    数导致难以训练，此外也会带来gradient vanish。为此何凯明大牛提出一种残差学习
    的网络架构，一方面实现了深层网络的学习与优化，另一方面也提高了模型的准确率


*具体实现*

    假设网络的某一层需要学习一个函数H(x), 假设H(x)可以使用F(x)+X的方式进行表示
    ，则使用后者来进行学习与处理。虽然两种表达方式所蕴含的意思是一样的，但是其
    对应网络的收敛速度与性能则是有天壤之别。 通过这样的转换，真正的实现了
    the deeper, the better。


*创新点与优势*

    优势：实现了构建更深层，性能更好的网络模型的目标。

    创新点：
    1。借鉴了图像处理中残差向量编码的思想，将残差这一概念引入到了深度学习中，从
    而取得了绝佳的性能

*实验结果*

    1. ImageNet Classification
    (1) Plain Network:34 层的plain网络比18层的plain网络性能更差。观测到了degradation问题(这不是
        由于vanishing gradients，因为在训练过程中使用了BN，推测原因是因为更深的
        网络其收敛的速率更低)

    (2) Residual Network 34层的网络性能比18层的网络性能更好(基本结构
        和之前的一模一样，不过是加了shortcut Connection)

    (3) Identity VS Projection Shortcuts
    进行了三种测试：
      A. zero-padding 被用来扩增维度，且所有的维度都没有参数
      B. projection shortcut被用来扩增维度，其他的使用identity shortcut
      C. 所有的shortcut都使用projection

      实验表明，C远好于B，B略好于A。但鉴于三者都没有非常明显的区别，所以为了减
      少参数以及复杂度，将不会使用C方法


*一点想法*

    1. 在论文中指出，34层的残差网络较50/101/152层更准确，这其实也可以反映出，并
    不完全是the deeper，the better。这个deeper应该是在一定范围内的。有理由推测
    ，尽管网络所要执行任务的不同，但是其最优的网络层数，大致上是在18-50之间的。
    （在此用二次函数来拟合）

    2. 另外就是在查阅资料中看到一个非常有意思的观点，把这个identity shortcut视
    为一个短路，这样的话，有可能网络在进行拟合时，有很多都是通过了identity 
    shortcut这一条路。如此看来，其实网络的真正层数就会大大降低。感觉这个从某种
    程度上也可以解释第一个猜测：其实并不是越深越好，只不过是152层的resnet看起来
    很深，但其真实的层数，也许也就相当于30多层的plain network。

    3.能不能把ResNet嵌入到SSD中？或许会有不错的表现


*SSD 相较YOLO速度提升--？*
*MobileNet 为什么会比较快*
*哪些结构更适合CPU*

